{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e76Widdi4ZCF"
   },
   "source": [
    "# Package install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIEB6APFqaMA",
    "outputId": "c7e4f4e8-8d32-449c-ede7-974e082573ff"
   },
   "outputs": [],
   "source": [
    "# install Hugging Face Libraries\n",
    "!pip install \"peft==0.2.0\"\n",
    "!pip install \"transformers==4.27.2\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n",
    "# install additional dependencies needed for training\n",
    "!pip install datasets\n",
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq5uDgwEb98Y"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daliM-Z9rL7p"
   },
   "source": [
    "\n",
    "# Process Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIGspDANQGjg"
   },
   "outputs": [],
   "source": [
    "dataset_id = \"minh21/cpgQA-v1.0-unique-context\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "5b48891dbe3b4e2f98a81bc5083babbc",
      "95804f2b2be6436cb717c6a8ecddcba8",
      "76fcf08f765f45e994a1a56aea0d9501",
      "75c2bb321cb7468c9c66406ffc195f12",
      "6f571c48ab2049ada8ab93af70a7453d",
      "88b4904119b54fc08cca8f228457491b",
      "f673c5ce9c9f4aa897cccf170ed0098f",
      "6366db57e805423bb3b1d34b4fcedea8",
      "24e2daaf97b64bdabbc5f58a2686a1aa",
      "3ab2912eb0664402b3dea4425943227e",
      "130fa30fe6874e958f943d57676678eb"
     ]
    },
    "id": "QXjoXMD4rU2g",
    "outputId": "7695a194-423b-4bcc-9d46-f8236e01b4b0"
   },
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"minh21/cpgQA-v1.0-unique-context-for-flan-t5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnrHBFzGrdHq"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0Wc0fVlrfvS",
    "outputId": "c115aee8-cdc4-4ff7-9040-bfc76fa8db00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'id', 'question', 'answer_text', 'answer_start', 'context'],\n",
       "    num_rows: 860\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XXbPU-HrlRr"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "# model_id=\"facebook/bart-large\"\n",
    "# Load tokenizer of FLAN-t5-large\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlEoRgDHroes"
   },
   "outputs": [],
   "source": [
    "# def tokenize(batch):\n",
    "#     inputs = tokenizer(f\"[CONTEXT]: {batch['context']} \\n [QUESTION]: {batch['question']}\" , padding= True, truncation = True, max_length = max_length, return_tensors='pt')\n",
    "#     targets = tokenizer(batch['answer_text'], padding= True, truncation = True, max_length = max_length, return_tensors='pt')\n",
    "#     inputs['labels'] = targets['input_ids']\n",
    "#     return inputs\n",
    "def tokenize(batch):\n",
    "    input = f\"\"\"\n",
    "    Read this and answer the question. If the question is unanswerable, \"say \\\"unanswerable\\\".\\n\\n{batch['context']}\\n\\n{batch['question']}\"\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input, padding=True, truncation=True, max_length=max_length)\n",
    "    targets = tokenizer(\n",
    "        batch[\"answer_text\"], padding=True, truncation=True, max_length=max_length\n",
    "    )\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ev7zDhNsCm3",
    "outputId": "f1cc30ca-9fb2-47e3-dada-f4c939047b2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/minh21___parquet/minh21--cpgQA-v1.0-unique-context-for-flan-t5-66322baccbdf5018/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-6243c1cae0ef1ab0.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/minh21___parquet/minh21--cpgQA-v1.0-unique-context-for-flan-t5-66322baccbdf5018/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-37e3399474ab6bc2.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize, batched=False, remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize, batched=False, remove_columns=test_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-cUa9MbB3mj",
    "outputId": "a5e17ce2-19dc-4b61-de74-f860c8080349"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qnBYPYosI8N",
    "outputId": "57a74350-f9bf-4fb4-9b15-71431dac9395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read this and answer the question. If the question is unanswerable, \"say \"unanswerable\". The Opioid Taper Decision Tool is designed to assist Primary Care providers in determining if an opioid taper is necessary for a specific patient, in performing the taper, and in providing follow-up and support during the taper. What is the purpose of Opioid Taper Decision Tool?\" \n"
     ]
    }
   ],
   "source": [
    "first_context_decoded = tokenizer.decode(\n",
    "    tokenized_train_dataset[0][\"input_ids\"], skip_special_tokens=True\n",
    ")\n",
    "print(first_context_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPE62M5csMC_",
    "outputId": "ef74d004-212f-43eb-c8be-3fe5b954ba94"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlNMSRVOsaHi"
   },
   "source": [
    "# Fine-tune and evaluate FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfNjxMYJsjC5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pSq3zXUWjOL",
    "outputId": "48dee120-61a3-4a65-b113-0a89d8e67d00"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=39,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1DJDqaqXI0Y"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, label_pad_token_id=label_pad_token_id, pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZArCVGUMXNdy"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir = \"lora-flan-t5-large\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,  # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    # eval_dataset = tokenized_test_dataset,\n",
    "    # compute_metrics = compute_metrics\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Vj18rc8KX9CV",
    "outputId": "5dc6f569-2399-47e3-8e3c-9f5720cff934"
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRPTbtlGcXJa"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\n",
    "    \"espiusedwards/flant5-large-lora\",\n",
    "    use_auth_token=True,\n",
    "    commit_message=\"not 8 bit, r = 39\",\n",
    "    private=True,\n",
    "    create_pr=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtzT-Dq_jPvv"
   },
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2QQmKDzeCNB"
   },
   "outputs": [],
   "source": [
    "# # Save our LoRA model & tokenizer results\n",
    "# peft_model_id=\"results\"\n",
    "# trainer.model.save_pretrained(peft_model_id)\n",
    "# tokenizer.save_pretrained(peft_model_id)\n",
    "# # if you want to save the base model to call\n",
    "# trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iep2_6YNfckT"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfH0EvSGe5Z1"
   },
   "outputs": [],
   "source": [
    "# Load adapters from hub\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc.\n",
    "peft_model_id = \"espiusedwards/flant5-large-lora\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    config.base_model_name_or_path, load_in_8bit=True, device_map={\"\": 0}\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\": 0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XO9rR80-MQhG"
   },
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "predictions_for_squad = []\n",
    "predictions_for_squad_v2 = []\n",
    "predictions_for_bert_score = []\n",
    "references_for_bert_score = []\n",
    "for data in test_dataset:\n",
    "    context = data[\"context\"]\n",
    "    question = data[\"question\"]\n",
    "    answer = data[\"answer_text\"]\n",
    "    id = data[\"id\"]\n",
    "    input = f\"\"\"\n",
    "    Read this and answer the question. If the question is unanswerable, \"say \\\"unanswerable\\\".\\n\\n{context}\\n\\n{question}\"\n",
    "    \"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        input,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(torch.device(\"cuda\"))\n",
    "\n",
    "    model_output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            input_ids=model_inputs[\"input_ids\"],\n",
    "            attention_mask=model_inputs[\"attention_mask\"],\n",
    "        )[0],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    predictions_for_squad.append(\n",
    "        {\n",
    "            \"prediction_text\": model_output,\n",
    "            \"id\": str(id),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    predictions_for_squad_v2.append(\n",
    "        {\n",
    "            \"prediction_text\": model_output,\n",
    "            \"no_answer_probability\": 0,\n",
    "            \"id\": str(id),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    predictions_for_bert_score.append(model_output)\n",
    "    references_for_bert_score.append(answer)\n",
    "    # predictions.extend(predicted_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dr8ulytQL4g_"
   },
   "outputs": [],
   "source": [
    "references_for_squad_v2 = [\n",
    "    {\n",
    "        \"answers\": {\"answer_start\": [ds[\"answer_start\"]], \"text\": [ds[\"answer_text\"]]},\n",
    "        \"id\": str(ds[\"id\"]),\n",
    "    }\n",
    "    for id, ds in enumerate(test_dataset)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-2YOxlNMZFj"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "results = dict()\n",
    "squad_metric = load(\"squad_v2\")\n",
    "results[\"squad_v2\"] = squad_metric.compute(\n",
    "    predictions=predictions_for_squad_v2, references=references_for_squad_v2\n",
    ")\n",
    "\n",
    "squad_metric = load(\"squad\")\n",
    "results[\"squad\"] = squad_metric.compute(\n",
    "    predictions=predictions_for_squad, references=references_for_squad_v2\n",
    ")\n",
    "\n",
    "bleu_metrics = load(\"bleu\")\n",
    "results[\"bleu\"] = bleu_metrics.compute(\n",
    "    predictions=predictions_for_bert_score, references=references_for_bert_score\n",
    ")\n",
    "\n",
    "bertscore_metric = load(\"bertscore\")\n",
    "berscore = bertscore_metric.compute(\n",
    "    predictions=predictions_for_bert_score,\n",
    "    references=references_for_bert_score,\n",
    "    lang=\"en\",\n",
    ")\n",
    "\n",
    "results[\"bertscore\"] = {\n",
    "    \"precision\": sum(berscore[\"precision\"]) / len(berscore[\"precision\"]),\n",
    "    \"recall\": sum(berscore[\"recall\"]) / len(berscore[\"recall\"]),\n",
    "    \"f1\": sum(berscore[\"f1\"]) / len(berscore[\"f1\"]),\n",
    "}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTHEYLk8tBPm"
   },
   "source": [
    "# Too small loss => Validation check on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D602sa46wc8"
   },
   "outputs": [],
   "source": [
    "# validation set\n",
    "validation_dataset = train_dataset.select(range(100))\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ro8578R7GuH"
   },
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "predictions_for_squad = []\n",
    "predictions_for_squad_v2 = []\n",
    "predictions_for_bert_score = []\n",
    "references_for_bert_score = []\n",
    "for data in validation_dataset:\n",
    "    context = data[\"context\"]\n",
    "    question = data[\"question\"]\n",
    "    answer = data[\"answer_text\"]\n",
    "    id = data[\"id\"]\n",
    "    input = f\"\"\"\n",
    "    Read this and answer the question. If the question is unanswerable, \"say \\\"unanswerable\\\".\\n\\n{context}\\n\\n{question}\"\n",
    "    \"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        input,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(torch.device(\"cuda\"))\n",
    "\n",
    "    model_output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            input_ids=model_inputs[\"input_ids\"],\n",
    "            attention_mask=model_inputs[\"attention_mask\"],\n",
    "        )[0],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    predictions_for_squad.append(\n",
    "        {\n",
    "            \"prediction_text\": model_output,\n",
    "            \"id\": str(id),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    predictions_for_squad_v2.append(\n",
    "        {\n",
    "            \"prediction_text\": model_output,\n",
    "            \"no_answer_probability\": 0,\n",
    "            \"id\": str(id),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    predictions_for_bert_score.append(model_output)\n",
    "    references_for_bert_score.append(answer)\n",
    "    # predictions.extend(predicted_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rspz9sgJ7OH-"
   },
   "outputs": [],
   "source": [
    "references_for_squad_v2 = [\n",
    "    {\n",
    "        \"answers\": {\"answer_start\": [ds[\"answer_start\"]], \"text\": [ds[\"answer_text\"]]},\n",
    "        \"id\": str(ds[\"id\"]),\n",
    "    }\n",
    "    for id, ds in enumerate(validation_dataset)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXu43JD27SBA"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "results = dict()\n",
    "squad_metric = load(\"squad_v2\")\n",
    "results[\"squad_v2\"] = squad_metric.compute(\n",
    "    predictions=predictions_for_squad_v2, references=references_for_squad_v2\n",
    ")\n",
    "\n",
    "squad_metric = load(\"squad\")\n",
    "results[\"squad\"] = squad_metric.compute(\n",
    "    predictions=predictions_for_squad, references=references_for_squad_v2\n",
    ")\n",
    "\n",
    "bleu_metrics = load(\"bleu\")\n",
    "results[\"bleu\"] = bleu_metrics.compute(\n",
    "    predictions=predictions_for_bert_score, references=references_for_bert_score\n",
    ")\n",
    "\n",
    "bertscore_metric = load(\"bertscore\")\n",
    "berscore = bertscore_metric.compute(\n",
    "    predictions=predictions_for_bert_score,\n",
    "    references=references_for_bert_score,\n",
    "    lang=\"en\",\n",
    ")\n",
    "\n",
    "results[\"bertscore\"] = {\n",
    "    \"precision\": sum(berscore[\"precision\"]) / len(berscore[\"precision\"]),\n",
    "    \"recall\": sum(berscore[\"recall\"]) / len(berscore[\"recall\"]),\n",
    "    \"f1\": sum(berscore[\"f1\"]) / len(berscore[\"f1\"]),\n",
    "}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_1uFvgItH4a"
   },
   "source": [
    "# Draft - Try functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LN4lCBZ3qHBq"
   },
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "# # Load the model and tokenizer\n",
    "# model = T5ForConditionalGeneration.from_pretrained('espiusedwards/flant5-large-lora')\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-small') # adjust model size if necessary\n",
    "# model.eval()\n",
    "\n",
    "# # Load the metric\n",
    "# metric = evaluate.load(\"squad_v2\")\n",
    "\n",
    "# # Assuming `test_data` is your test data\n",
    "# # and test_data is a list of dictionaries with 'question', 'context', and 'id' keys\n",
    "\n",
    "# # Get predictions\n",
    "# predictions = []\n",
    "# for i in range(len(tokenized_test_dataset)):\n",
    "#     item = tokenized_test_dataset[i]\n",
    "#     inputs = {'input_ids': item['input_ids'], 'attention_mask': item['attention_mask']}\n",
    "#     outputs = model.generate(**inputs, max_length=512, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "#     prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     predictions.append({'prediction_text': prediction, 'id': item['id']})\n",
    "\n",
    "# references = [{'answers': {'answer_start': [0], 'text': [item['answer']]}, 'id': item['id']} for item in tokenized_test_dataset]\n",
    "\n",
    "# # Compute the metric\n",
    "# result = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# # Display the result\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qxl81xZDfVGB"
   },
   "outputs": [],
   "source": [
    "# # evaluate\n",
    "# import evaluate\n",
    "# import numpy as np\n",
    "# from datasets import load_from_disk\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Metric\n",
    "# metric = evaluate.load(\"squad_v2\")\n",
    "\n",
    "# def evaluate_peft_model(sample,max_target_length=50):\n",
    "#     # generate summary\n",
    "#     outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)\n",
    "#     prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "#     # decode eval sample\n",
    "#     # Replace -100 in the labels as we can't decode them.\n",
    "#     labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "#     labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#     # Some simple post-processing\n",
    "#     return prediction, labels\n",
    "\n",
    "# # dataset: test_dataset\n",
    "\n",
    "# # run predictions\n",
    "# # this can take ~45 minutes\n",
    "# predictions, references = [] , []\n",
    "# for sample in tqdm(test_dataset):\n",
    "#     p,l = evaluate_peft_model(sample)\n",
    "#     predictions.append(p)\n",
    "#     references.append(l)\n",
    "\n",
    "# # compute metric\n",
    "# squad_v2 = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# # print results\n",
    "# print(f\"Exact: {squad_v2['exact']}\")\n",
    "# print(f\"f1: {squad_v2['f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSx85AbT6ZvZ"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import HfFolder\n",
    "# from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "# # Hugging Face repository id\n",
    "# repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n",
    "# # Define training args\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=repository_id,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     predict_with_generate=True,\n",
    "#     fp16=False, # Overflows with fp16\n",
    "#     learning_rate=5e-5,\n",
    "#     num_train_epochs=5,\n",
    "#     # logging & evaluation strategies\n",
    "#     logging_dir=f\"{repository_id}/logs\",\n",
    "#     logging_strategy=\"steps\",\n",
    "#     logging_steps=500,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     save_total_limit=2,\n",
    "#     load_best_model_at_end=True,\n",
    "#     # metric_for_best_model=\"overall_f1\",\n",
    "#     # push to hub parameters\n",
    "#     report_to=\"tensorboard\",\n",
    "#     push_to_hub=False,\n",
    "#     hub_strategy=\"every_save\",\n",
    "#     #hub_model_id=repository_id,\n",
    "#     hub_token=HfFolder.get_token(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFliRKs14woL"
   },
   "outputs": [],
   "source": [
    "# Compute metric\n",
    "# import evaluate\n",
    "# import numpy as np\n",
    "\n",
    "# # Metric\n",
    "# metric = evaluate.load(\"squad_v2\")\n",
    "\n",
    "# def compute_metrics(eval_preds):\n",
    "#     preds, labels = eval_preds\n",
    "#     if isinstance(preds, tuple):\n",
    "#         preds = preds[0]\n",
    "\n",
    "#     # Convert predictions and labels to the format expected by squad_v2 metric\n",
    "#     predictions = [{'prediction_text': pred, 'id': i} for i, pred in enumerate(preds)]\n",
    "#     references = [{'answers': {'answer_start': [0], 'text': [label]}, 'id': i} for i, label in enumerate(labels)]\n",
    "\n",
    "#     result = metric.compute(predictions=predictions, references=references)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqcebRpR-EAq"
   },
   "outputs": [],
   "source": [
    "# Compute metric\n",
    "# import evaluate\n",
    "# import numpy as np\n",
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "# # Metric\n",
    "# metric = evaluate.load(\"squad\")\n",
    "\n",
    "# def compute_metrics(eval_preds):\n",
    "#     preds, labels = eval_preds\n",
    "#     if isinstance(preds, tuple):\n",
    "#         preds = preds[0]\n",
    "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "#     # Replace -100 in the labels as we can't decode them.\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#     result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "#     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "#     result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQ4Hbuq98qw1"
   },
   "outputs": [],
   "source": [
    "# # Create Trainer instance\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=tokenized_train_dataset,\n",
    "#     eval_dataset=tokenized_test_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DTHEYLk8tBPm",
    "M_1uFvgItH4a"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "130fa30fe6874e958f943d57676678eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24e2daaf97b64bdabbc5f58a2686a1aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ab2912eb0664402b3dea4425943227e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b48891dbe3b4e2f98a81bc5083babbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_95804f2b2be6436cb717c6a8ecddcba8",
       "IPY_MODEL_76fcf08f765f45e994a1a56aea0d9501",
       "IPY_MODEL_75c2bb321cb7468c9c66406ffc195f12"
      ],
      "layout": "IPY_MODEL_6f571c48ab2049ada8ab93af70a7453d"
     }
    },
    "6366db57e805423bb3b1d34b4fcedea8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f571c48ab2049ada8ab93af70a7453d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75c2bb321cb7468c9c66406ffc195f12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ab2912eb0664402b3dea4425943227e",
      "placeholder": "​",
      "style": "IPY_MODEL_130fa30fe6874e958f943d57676678eb",
      "value": " 2/2 [00:00&lt;00:00, 33.03it/s]"
     }
    },
    "76fcf08f765f45e994a1a56aea0d9501": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6366db57e805423bb3b1d34b4fcedea8",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_24e2daaf97b64bdabbc5f58a2686a1aa",
      "value": 2
     }
    },
    "88b4904119b54fc08cca8f228457491b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95804f2b2be6436cb717c6a8ecddcba8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88b4904119b54fc08cca8f228457491b",
      "placeholder": "​",
      "style": "IPY_MODEL_f673c5ce9c9f4aa897cccf170ed0098f",
      "value": "100%"
     }
    },
    "f673c5ce9c9f4aa897cccf170ed0098f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
