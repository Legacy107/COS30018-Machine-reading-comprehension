{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhNZ5Lp8mSOM"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft transformers datasets evaluate peft -q accelerate\n",
        "!pip install huggingface_hub\n",
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5nL3eHFBy9RC"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_config,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    set_peft_model_state_dict,\n",
        "    PeftType,\n",
        "    PromptEncoderConfig,\n",
        ")\n",
        "import torch\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
        "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXFh15l3PvGK"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0O3CjaZkmPM1"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('minh21/cpgQA-v1.0-unique-context-for-flan-t5')\n",
        "device = \"cuda\"\n",
        "model_name='google/flan-t5-large'\n",
        "lr = 3e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sVqJ5nVQmuH0"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qVTS1CLEcpVL"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) #Convert text to vector space\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X6ljA7IbfeZ"
      },
      "source": [
        "# Zero shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQYuGBamcxVL"
      },
      "source": [
        "Test the tokenizer encoding and decoding a simple sentence:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bf_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "wTMm4Al0AVHH"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUSTosFTpns5",
        "outputId": "ad9ec91c-c450-4e73-b432-735c9c5a33aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "def generate_predictions(example):\n",
        "    question = example[\"question\"]\n",
        "    context = example[\"context\"]\n",
        "    id = example['id']\n",
        "    prompt = f\"\"\"\n",
        "    Read this and answer the question. If the question is unanswerable, \"\n",
        "    say \\\"unanswerable\\\".\\n\\n{context}\\n\\n{question}\",\n",
        "    \"\"\"\n",
        "\n",
        "    # Input constructed prompt instead of the dialogue.\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        bf_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    answer = {\n",
        "        'prediction_text': output,\n",
        "        'no_answer_probability': 0,\n",
        "        'id': str(id)\n",
        "    }\n",
        "    return answer\n",
        "\n",
        "predictions = []\n",
        "for i, index in enumerate(test_dataset):\n",
        "    # predictions.append(index)\n",
        "    predictions.append(generate_predictions(index))\n",
        "\n",
        "\n",
        "predictions.__len__()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ0ViVaCPeMx"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po-MlWGR-l70",
        "outputId": "1a9f98fc-2b9c-4d4c-b575-cef2d3093c19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact': 59.72222222222222,\n",
              " 'f1': 82.08038451752687,\n",
              " 'total': 144,\n",
              " 'HasAns_exact': 59.72222222222222,\n",
              " 'HasAns_f1': 82.08038451752687,\n",
              " 'HasAns_total': 144,\n",
              " 'best_exact': 59.72222222222222,\n",
              " 'best_exact_thresh': 0.0,\n",
              " 'best_f1': 82.08038451752687,\n",
              " 'best_f1_thresh': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "from evaluate import load\n",
        "squad_metric = load(\"squad_v2\")\n",
        "results = squad_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh--LRmETcZ7"
      },
      "source": [
        "# One shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ-5rg5MTf0O"
      },
      "outputs": [],
      "source": [
        "def generate_predictions_one_shot(example, example2):\n",
        "    question_train = example2[\"question\"]\n",
        "    context_train = example2[\"context\"]\n",
        "    answer_text_train = example2[\"answer_text\"]\n",
        "\n",
        "    question = example[\"question\"]\n",
        "    context = example[\"context\"]\n",
        "    id = example['id']\n",
        "    prompt = f\"\"\"\n",
        "    Using this example:\n",
        "    [EXAMPLE]Read this and answer the question. If the question is unanswerable, \"\n",
        "    say \\\"unanswerable\\\".\\n\\n{context_train}\\n\\n{question_train}\", \"{answer_text_train}\"\n",
        "\n",
        "    To do\n",
        "    Read this and answer the question. If the question is unanswerable, \"\n",
        "    say \\\"unanswerable\\\".\\n\\n{context}\\n\\n{question}\",\n",
        "    \"\"\"\n",
        "\n",
        "    # Input constructed prompt instead of the dialogue.\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    answer = {\n",
        "        'prediction_text': output,\n",
        "        'no_answer_probability': 0,\n",
        "        'id': str(id)\n",
        "    }\n",
        "    return answer\n",
        "\n",
        "predictions_one_shot = []\n",
        "for i, index in enumerate(test_dataset):\n",
        "    if(i>0):\n",
        "      predictions_one_shot.append(generate_predictions_one_shot(test_dataset[i], test_dataset[i-1]))\n",
        "    else:\n",
        "      predictions_one_shot.append(generate_predictions_one_shot(test_dataset[i], test_dataset[test_dataset.__len__()-1]))\n",
        "\n",
        "\n",
        "predictions_one_shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zQ-txZXfn4v",
        "outputId": "7dde2e20-f3a5-4d5b-b4aa-e5bb51fbd7b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'exact': 51.81818181818182,\n",
              " 'f1': 75.3573430317572,\n",
              " 'total': 110,\n",
              " 'HasAns_exact': 51.81818181818182,\n",
              " 'HasAns_f1': 75.3573430317572,\n",
              " 'HasAns_total': 110,\n",
              " 'best_exact': 51.81818181818182,\n",
              " 'best_exact_thresh': 0.0,\n",
              " 'best_f1': 75.3573430317572,\n",
              " 'best_f1_thresh': 0.0}"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from evaluate import load\n",
        "squad_metric = load(\"squad_v2\")\n",
        "results = squad_metric.compute(predictions=predictions_one_shot, references=references)\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgpuIr0Wfxao"
      },
      "outputs": [],
      "source": [
        "def generate_predictions_one_shot(example, example2):\n",
        "    question_train = example2[\"question\"]\n",
        "    context_train = example2[\"context\"]\n",
        "    answer_text_train = example2[\"answer_text\"]\n",
        "\n",
        "    question = example[\"question\"]\n",
        "    context = example[\"context\"]\n",
        "    id = example['id']\n",
        "    prompt = f\"\"\"\n",
        "    Using this example:\n",
        "    [CONTEXT]: {context_train}\\n\n",
        "    [QUESTION]: {question_train} \\n\n",
        "    [ANSWER]: {answer_text_train} \\n\n",
        "\n",
        "    [CONTEXT]: {context}\\n\n",
        "    [QUESTION]: {question} \\n\n",
        "    [ANSWER]:\n",
        "    \"\"\"\n",
        "\n",
        "    # Input constructed prompt instead of the dialogue.\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    answer = {\n",
        "        'prediction_text': output,\n",
        "        'no_answer_probability': 0,\n",
        "        'id': str(id)\n",
        "    }\n",
        "    return answer\n",
        "\n",
        "predictions_one_shot = []\n",
        "for i, index in enumerate(test_dataset):\n",
        "    if(i>0):\n",
        "      predictions_one_shot.append(generate_predictions_one_shot(test_dataset[i], test_dataset[i-1]))\n",
        "    else:\n",
        "      predictions_one_shot.append(generate_predictions_one_shot(test_dataset[i], test_dataset[test_dataset.__len__()-1]))\n",
        "\n",
        "\n",
        "predictions_one_shot\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntTz5X1Sm8j7",
        "outputId": "ecbe01bb-4258-4d31-f2ef-f8af7669b0a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'exact': 55.45454545454545,\n",
              " 'f1': 79.78866711083056,\n",
              " 'total': 110,\n",
              " 'HasAns_exact': 55.45454545454545,\n",
              " 'HasAns_f1': 79.78866711083056,\n",
              " 'HasAns_total': 110,\n",
              " 'best_exact': 55.45454545454545,\n",
              " 'best_exact_thresh': 0.0,\n",
              " 'best_f1': 79.78866711083056,\n",
              " 'best_f1_thresh': 0.0}"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from evaluate import load\n",
        "squad_metric = load(\"squad_v2\")\n",
        "results = squad_metric.compute(predictions=predictions_one_shot, references=references)\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icyKGkD6nBQX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x65z4x0tnBPS"
      },
      "source": [
        "# Several shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ga0DOmtVnJk0"
      },
      "outputs": [],
      "source": [
        "def generate_predictions_several_shots(example, example2, example3, example4):\n",
        "    question_train_2 = example2[\"question\"]\n",
        "    context_train_2 = example2[\"context\"]\n",
        "    answer_text_train_2 = example2[\"answer_text\"]\n",
        "\n",
        "    question_train_3 = example3[\"question\"]\n",
        "    context_train_3 = example3[\"context\"]\n",
        "    answer_text_train_3 = example3[\"answer_text\"]\n",
        "\n",
        "    question_train_4 = example4[\"question\"]\n",
        "    context_train_4 = example4[\"context\"]\n",
        "    answer_text_train_4 = example4[\"answer_text\"]\n",
        "\n",
        "    question = example[\"question\"]\n",
        "    context = example[\"context\"]\n",
        "    id = example['id']\n",
        "    prompt = f\"\"\"\n",
        "    Using this example:\n",
        "    [CONTEXT]: {context_train_2}\\n\n",
        "    [QUESTION]: {question_train_2}\\n\n",
        "    [ANSWER]: {answer_text_train_2}\\n\n",
        "\n",
        "    [CONTEXT]: {context_train_3}\\n\n",
        "    [QUESTION]: {question_train_3}\\n\n",
        "    [ANSWER]: {answer_text_train_3}\\n\n",
        "\n",
        "    [CONTEXT]: {context_train_4}\\n\n",
        "    [QUESTION]: {question_train_4}\\n\n",
        "    [ANSWER]: {answer_text_train_4}\\n\n",
        "\n",
        "    [CONTEXT]: {context}\\n\n",
        "    [QUESTION]: {question} \\n\n",
        "    [ANSWER]:\n",
        "    \"\"\"\n",
        "\n",
        "    # Input constructed prompt instead of the dialogue.\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    answer = {\n",
        "        'prediction_text': output,\n",
        "        'no_answer_probability': 0,\n",
        "        'id': str(id)\n",
        "    }\n",
        "    return answer\n",
        "\n",
        "predictions_several_shots = []\n",
        "for i, index in enumerate(test_dataset):\n",
        "    if(i>3):\n",
        "      predictions_several_shots.append(generate_predictions_several_shots(test_dataset[i], test_dataset[i-1], test_dataset[i-2], test_dataset[i-3]))\n",
        "    else:\n",
        "      predictions_several_shots.append(generate_predictions_several_shots(test_dataset[i], test_dataset[test_dataset.__len__()-i-1], test_dataset[test_dataset.__len__()-i-2], test_dataset[test_dataset.__len__()-i-3] ))\n",
        "\n",
        "\n",
        "predictions_several_shots\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJIJtPFc4UeT",
        "outputId": "d629259c-d1dd-4c98-b6a0-95dfb0fab397"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'exact': 55.45454545454545,\n",
              " 'f1': 79.22830556919165,\n",
              " 'total': 110,\n",
              " 'HasAns_exact': 55.45454545454545,\n",
              " 'HasAns_f1': 79.22830556919165,\n",
              " 'HasAns_total': 110,\n",
              " 'best_exact': 55.45454545454545,\n",
              " 'best_exact_thresh': 0.0,\n",
              " 'best_f1': 79.22830556919165,\n",
              " 'best_f1_thresh': 0.0}"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from evaluate import load\n",
        "squad_metric = load(\"squad_v2\")\n",
        "results = squad_metric.compute(predictions=predictions_several_shots, references=references)\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4VRM7D50CB8"
      },
      "source": [
        "# Prompt Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IEujzTia0HCe"
      },
      "outputs": [],
      "source": [
        "peft_config = PromptTuningConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
        "    num_virtual_tokens=20,\n",
        "    prompt_tuning_init_text=\"Answer this question using the provided context\",\n",
        "    tokenizer_name_or_path=model_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1e3O6dThFMSH"
      },
      "outputs": [],
      "source": [
        "model_max_length = tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPgrj4fQ-273",
        "outputId": "d6b5d9e1-95e4-4ff8-8bcc-c2ba399b3965"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "481"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# target_max_length = max([len(tokenizer(data['context'])[\"input_ids\"]) for data in train_dataset])\n",
        "\n",
        "maxx = 0\n",
        "dat = []\n",
        "for data in train_dataset:\n",
        "  text = data['context'] + \"\\n\" + data['question']\n",
        "  l = tokenizer(text)[\"input_ids\"].__len__()\n",
        "  if l>=512:\n",
        "    dat.append(data)\n",
        "  if l >= maxx:\n",
        "    maxx = l\n",
        "dat\n",
        "maxx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5yUZvFLAB8Kd"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(data):\n",
        "    context = data['context']\n",
        "    question = data['question']\n",
        "    answer = data['answer_text']\n",
        "    id = data['id']\n",
        "    input = f\"\"\"\n",
        "    Read this and answer the question. If the question is unanswerable, \"\n",
        "    say \\\"unanswerable\\\".\\n\\n{context}\\n\\n{question}\",\n",
        "    \"\"\"\n",
        "    model_inputs = tokenizer(\n",
        "        input,\n",
        "        padding='max_length',\n",
        "        max_length=512,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(answer)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    labels['id'] = id\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFTOLXv4cV8a"
      },
      "outputs": [],
      "source": [
        "processed_datasets = {\n",
        "    'train': train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=False,\n",
        "    num_proc=1,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on train_dataset\",\n",
        "    ),\n",
        "    'test': test_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=False,\n",
        "    num_proc=1,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on test_dataset\",\n",
        "  )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCCt-ctChgvY",
        "outputId": "7f59b747-0bdd-42d6-b4ff-761f7a20e743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable model parameters: 40960\n",
            "all model parameters: 783191040\n",
            "percentage of trainable model parameters: 0.01%\n",
            "trainable params: 40,960 || all params: 783,191,040 || trainable%: 0.005229886184601908\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model = get_peft_model(model, peft_config)\n",
        "label_pad_token_id = -100\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8\n",
        ")\n",
        "\n",
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aNIIn_nGiU4P"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=lr,\n",
        "    do_eval=False,\n",
        "    output_dir=\"./flan-t5-large-prompt-tuning-cpgQA\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    push_to_hub=True,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=200,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tqesa6iInr-2"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_datasets[\"train\"],\n",
        "    eval_dataset=processed_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    # optimizers=[optimizer, lr_scheduler],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "v_jjo3z6CMaR",
        "outputId": "23fbe5cd-0e79-425f-f6bf-b2fe0b7b5cd7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2150' max='2150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2150/2150 29:56, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.192000</td>\n",
              "      <td>0.136314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.240200</td>\n",
              "      <td>0.136312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.136309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.224300</td>\n",
              "      <td>0.136309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.224300</td>\n",
              "      <td>0.136308</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2150, training_loss=0.22789144649062046, metrics={'train_runtime': 1800.7615, 'train_samples_per_second': 2.388, 'train_steps_per_second': 1.194, 'total_flos': 9910502188646400.0, 'train_loss': 0.22789144649062046, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mo0cVfY-Emwp",
        "outputId": "ef59e98f-24ce-490a-a397-d0ae886ccc9c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://huggingface.co/minh21/flan-t5-large-prompt-tuning-cpgQA/tree/main/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model from hub"
      ],
      "metadata": {
        "id": "T6kBqK9d1jys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load adapters from hub\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"minh21/flan-t5-large-prompt-tuning-cpgQA\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n",
        "print(\"Peft model loaded\")"
      ],
      "metadata": {
        "id": "fdkuE57ZfUu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation model"
      ],
      "metadata": {
        "id": "pRVYoROO1mw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cuda')\n",
        "predictions_for_squad = []\n",
        "predictions_for_squad_v2 = []\n",
        "predictions_for_bert_score = []\n",
        "references_for_bert_score = []\n",
        "for data in test_dataset:\n",
        "    context = data['context']\n",
        "    question = data['question']\n",
        "    answer = data['answer_text']\n",
        "    id = data['id']\n",
        "    input = f\"\"\"\n",
        "    \"{context}\\nTry to answer this question if possible (otherwise reply \"\n",
        "        \"\\\"unanswerable\\\"): {question}\"\n",
        "    \"\"\"\n",
        "    model_inputs = tokenizer(\n",
        "        input,\n",
        "        padding='max_length',\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    ).to(torch.device(\"cuda\"))\n",
        "\n",
        "\n",
        "    model_output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            input_ids=model_inputs[\"input_ids\"],\n",
        "            attention_mask=model_inputs[\"attention_mask\"],\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    predictions_for_squad.append({\n",
        "          'prediction_text': model_output,\n",
        "          'id': str(id),\n",
        "    })\n",
        "\n",
        "    predictions_for_squad_v2.append({\n",
        "          'prediction_text': model_output,\n",
        "          'no_answer_probability': 0,\n",
        "          'id': str(id),\n",
        "    })\n",
        "\n",
        "    predictions_for_bert_score.append(model_output)\n",
        "    references_for_bert_score.append(answer)\n",
        "    # predictions.extend(predicted_texts)"
      ],
      "metadata": {
        "id": "EQYG62zugViN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "references_for_squad_v2 = [\n",
        "    {\n",
        "        \"answers\": {\"answer_start\": [ds[\"answer_start\"]], \"text\": [ds[\"answer_text\"]]},\n",
        "        \"id\": str(ds[\"id\"]),\n",
        "    }\n",
        "    for id, ds in enumerate(test_dataset)\n",
        "]"
      ],
      "metadata": {
        "id": "f8Xl8bC5S5mZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "results = dict()\n",
        "squad_metric = load(\"squad_v2\")\n",
        "results[\"squad_v2\"] = squad_metric.compute(predictions=predictions_for_squad_v2, references=references_for_squad_v2)\n",
        "\n",
        "squad_metric = load(\"squad\")\n",
        "results[\"squad\"] = squad_metric.compute(predictions=predictions_for_squad, references=references_for_squad_v2)\n",
        "\n",
        "bleu_metrics = load(\"bleu\")\n",
        "prediction = [\"hello there general kenobi\",\"foo bar foobar\"]\n",
        "reference = [\n",
        "[\"hello there general kenobi\"],\n",
        "[\"foo bar foobar\"]\n",
        "]\n",
        "results[\"bleu\"] = bleu_metrics.compute(predictions=predictions_for_bert_score, references=references_for_bert_score)\n",
        "\n",
        "bertscore_metric = load(\"bertscore\")\n",
        "berscore = bertscore_metric.compute(\n",
        "    predictions=predictions_for_bert_score, references=references_for_bert_score, lang=\"en\"\n",
        ")\n",
        "\n",
        "results[\"bertscore\"] = {\n",
        "    \"precision\": sum(berscore[\"precision\"]) / len(berscore[\"precision\"]),\n",
        "    \"recall\": sum(berscore[\"recall\"]) / len(berscore[\"recall\"]),\n",
        "    \"f1\": sum(berscore[\"f1\"]) / len(berscore[\"f1\"]),\n",
        "}\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEPAvHsDUjmx",
        "outputId": "b93e827e-dfb8-4973-e36a-fd6c5a13c322"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'squad_v2': {'exact': 47.916666666666664,\n",
              "  'f1': 75.94572464040252,\n",
              "  'total': 144,\n",
              "  'HasAns_exact': 47.916666666666664,\n",
              "  'HasAns_f1': 75.94572464040252,\n",
              "  'HasAns_total': 144,\n",
              "  'best_exact': 47.916666666666664,\n",
              "  'best_exact_thresh': 0.0,\n",
              "  'best_f1': 75.94572464040252,\n",
              "  'best_f1_thresh': 0.0},\n",
              " 'squad': {'exact_match': 47.916666666666664, 'f1': 75.94572464040252},\n",
              " 'bleu': {'bleu': 0.4510316021376081,\n",
              "  'precisions': [0.9239704329461457,\n",
              "   0.9028642590286425,\n",
              "   0.886094674556213,\n",
              "   0.8792184724689165],\n",
              "  'brevity_penalty': 0.5023340369535013,\n",
              "  'length_ratio': 0.592245153220763,\n",
              "  'translation_length': 947,\n",
              "  'reference_length': 1599},\n",
              " 'bertscore': {'precision': 0.9606295786798,\n",
              "  'recall': 0.9368742116623454,\n",
              "  'f1': 0.9481206101675829}}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3X6ljA7IbfeZ",
        "zh--LRmETcZ7",
        "x65z4x0tnBPS",
        "T6kBqK9d1jys",
        "pRVYoROO1mw8"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}